{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, regularizers, optimizers, activations, Model\n",
    "import scipy as sy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, Dense, ReLU, LayerNormalization, Dropout, Input\n",
    "from keras.backend import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import matmul, math, cast, float32, reshape, transpose, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import convert_to_tensor, string\n",
    "from keras.layers import TextVectorization, Embedding\n",
    "from tensorflow import data\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Score the queres against the keys after transposing the latter, scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True)/ math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        #apply mask to the attention scores \n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        #Compute the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        #Coimputing the attention by a weighted sum ofthe value vectors\n",
    "\n",
    "        return matmul(weights, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "batch_size = 64  # Batch size from the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vocab_size = 10\\ninput_seq_length = 5  # Maximum length of the input sequence\\noutput_sequence_length = 5\\nqueries = random.random((batch_size, input_seq_length, d_k))\\nkeys = random.random((batch_size, input_seq_length, d_k))\\nvalues = random.random((batch_size, input_seq_length, d_v))'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''vocab_size = 10\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "output_sequence_length = 5\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention = DotProductAttention()\n",
    "#print(attention(queries, keys, values, d_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention() #Scaled dot product attention\n",
    "        self.heads = h #number of attention heads to use\n",
    "        self.d_k = d_k # dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v # dimensionality of the linearly projected values\n",
    "        self.d_model = d_model #Dimensionality of the model\n",
    "        self.W_q = Dense(d_k) # Learned projectino matrix for the queries\n",
    "        self.W_k = Dense(d_k) # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v) # Learning projection matrix for the values\n",
    "        self.W_o = Dense(d_model) # learning projection matrix for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            #Tensor shape after reshaping and transpoing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, int(x.shape[2]/heads)))\n",
    "            x = transpose(x, perm=(0,2,1,3))\n",
    "\n",
    "        else:\n",
    "            #reverting the reshaping and transpoing operations: (batch_size, seq_length, d_model)\n",
    "            x = transpose(x, perm=(0,2,1,3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], int(x.shape[2]*x.shape[3])))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        #Rearrange the values to be able to compute all the heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        #resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshape queries, keys, and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # rearranage back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, self.heads, d_v)\n",
    "\n",
    "        # apply one final linear projection to the output to generate the multi-head attention\n",
    "        # resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "#print(multihead_attention(queries, keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the positional embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=100000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff) #first fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model) # Second fully connected layer\n",
    "        self.activation = ReLU() # ReLU activation layer\n",
    "\n",
    "    def call(self, x):\n",
    "        #the input is passed into the fully connected layers, with a relu inbetween\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization() #Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # the sublayer input and output need to be of the same shape to be summed\n",
    "\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        return self.layer_norm(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, sequence_length,h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.build(input_shape=[None, sequence_length, d_model])\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def build_graph(self):\n",
    "        input_layer = Input(shape=(self.sequence_length, self.d_model))\n",
    "        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x,x, padding_mask)\n",
    "        #Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "        #folling by an add and normalization layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "\n",
    "        #followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "\n",
    "        #add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "        #followed by another add and norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(sequence_length,h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        #Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        #add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nh = 8  # Number of self-attention heads\\nd_k = 64  # Dimensionality of the linearly projected queries and keys\\nd_v = 64  # Dimensionality of the linearly projected values\\nd_ff = 2048  # Dimensionality of the inner fully connected layer\\nd_model = 512  # Dimensionality of the model sub-layers' outputs\\nn = 6  # Number of layers in the encoder stack\\n\\nbatch_size = 64  # Batch size from the training process\\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\\n\\n\\nenc_vocab_size = 20 # Vocabulary size for the encoder\\ninput_seq_length = 5  # Maximum length of the input sequence\\n\\ninput_seq = random.random((batch_size, input_seq_length))\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the encoder layer\n",
    "'''\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(Layer):\n",
    "    def __init__(self,sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "        self.add_norm3 = AddNormalization()\n",
    "        self.build(input_shape=[None, sequence_length, d_model])\n",
    "        self.sequence_length = sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def build_graph(self):\n",
    "        input_layer = Input(shape=(self.sequence_length, self.d_model))\n",
    "        return Model(inputs=[input_layer], outputs=self.call(input_layer, input_layer, None, None, True))\n",
    "    \n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
    "        #Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        #add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    "\n",
    "        #followed by the add and norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "\n",
    "        #followed by another multi-head attention layer\n",
    "        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n",
    "\n",
    "        #another dropout\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    "\n",
    "        #another add and norm layer\n",
    "        addnorm_output2 = self.add_norm2(addnorm_output1, multihead_output2)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)\n",
    "\n",
    "        #and another dropout\n",
    "        feedforward_output = self.dropout3(feedforward_output, training=training)\n",
    "\n",
    "        #followed by another add and norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.decoder_layer = [DecoderLayer(sequence_length,h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        #Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(output_target)\n",
    "        #Epected output shape = (number of sentences, sequence_length, d_model)\n",
    "\n",
    "        # add in a dropout layer\n",
    "\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        #pass on the positional encoded values to each decoder layer\n",
    "        for i, layer in enumerate(self.decoder_layer):\n",
    "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nh = 8  # Number of self-attention heads\\nd_k = 64  # Dimensionality of the linearly projected queries and keys\\nd_v = 64  # Dimensionality of the linearly projected values\\nd_ff = 2048  # Dimensionality of the inner fully connected layer\\nd_model = 512  # Dimensionality of the model sub-layers' outputs\\nn = 6  # Number of layers in the encoder stack\\n\\nbatch_size = 64  # Batch size from the training process\\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\\n\\n\\ndec_vocab_size = 20 # Vocabulary size for the decoder\\ninput_seq_length = 5  # Maximum length of the input sequence\\n\\ninput_seq = random.random((batch_size, input_seq_length))\\nenc_output = random.random((batch_size, input_seq_length, d_model))\\n\\n\\ndecoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\\nprint(decoder(input_seq, enc_output, None, True))\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the decoder layer\n",
    "'''\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "\n",
    "dec_vocab_size = 20 # Vocabulary size for the decoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "enc_output = random.random((batch_size, input_seq_length, d_model))\n",
    "\n",
    "\n",
    "decoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(decoder(input_seq, enc_output, None, True))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together  + masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(input):\n",
    "    #create a mask which marks the zero padding values in the input by a 1\n",
    "    mask = math.equal(input, 0)\n",
    "    mask = cast(mask, float32)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. 0. 1. 1. 1.], shape=(7,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input = np.array([1,2,3,4,0,0,0])\n",
    "\n",
    "print(padding_mask(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import linalg, ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookahead_mask(shape):\n",
    "    # mask out the future entries by marking them with a 1.0\n",
    "    mask = 1 - linalg.band_part(ones((shape, shape)),-1,0)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(lookahead_mask(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(Model):\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n",
    "        super(TransformerModel, self).__init__(**kwargs)\n",
    "\n",
    "        #set up the encoder\n",
    "        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
    "\n",
    "        #set up the decoder\n",
    "        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
    "\n",
    "        #the final dense layer\n",
    "        self.model_last_layer = Dense(dec_vocab_size)\n",
    "\n",
    "    def padding_mask(self, input):\n",
    "        # Create mask which marks the zero padding values in the input by a 1.0\n",
    "        mask = math.equal(input, 0)\n",
    "        mask = cast(mask, float32)\n",
    " \n",
    "        # The shape of the mask should be broadcastable to the shape\n",
    "        # of the attention weights that it will be masking later on\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    " \n",
    "    def lookahead_mask(self, shape):\n",
    "        # Mask out future entries by marking them with a 1.0\n",
    "        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n",
    " \n",
    "        return mask\n",
    "    def call(self, encoder_input, decoder_input, training):\n",
    "\n",
    "        #create a padding mask to mask the encoder inputs and the encoder outputs in the decoder\n",
    "        enc_padding_mask = self.padding_mask(encoder_input)\n",
    "\n",
    "        #Create and conbine the padding and look-ahead masks to be fed into the decoder\n",
    "        dec_in_padding_mask = self.padding_mask(decoder_input)\n",
    "        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n",
    "        dec_in_lookahead_mask = math.maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n",
    "\n",
    "        #feed the input into the encoder\n",
    "        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n",
    "\n",
    "        #feed the encoder output into the decoder\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n",
    "\n",
    "        #pass the deocder output through the final dense layer\n",
    "        model_output = self.model_last_layer(decoder_output)\n",
    "\n",
    "        return model_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an instance of the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "enc_seq_length = 20  # Encoder sequence length\n",
    "dec_seq_length = 75  # Decoder sequence length\n",
    "enc_vocab_size = 2404  # Encoder vocabulary size\n",
    "dec_vocab_size = 3864  # Decoder vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (Multi  (None, 20, 512)     131776      ['input_1[0][0]',                \n",
      " HeadAttention)                                                   'input_1[0][0]',                \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 20, 512)      0           ['multi_head_attention_18[0][0]']\n",
      "                                                                                                  \n",
      " add_normalization_30 (AddNorma  (None, 20, 512)     1024        ['input_1[0][0]',                \n",
      " lization)                                                        'dropout_32[0][0]']             \n",
      "                                                                                                  \n",
      " feed_forward_12 (FeedForward)  (None, 20, 512)      2099712     ['add_normalization_30[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 20, 512)      0           ['feed_forward_12[0][0]']        \n",
      "                                                                                                  \n",
      " add_normalization_31 (AddNorma  (None, 20, 512)     1024        ['add_normalization_30[0][0]',   \n",
      " lization)                                                        'dropout_33[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,233,536\n",
      "Trainable params: 2,233,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
    "encoder.build_graph().summary()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 75, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_19 (Multi  (None, 75, 512)     131776      ['input_2[0][0]',                \n",
      " HeadAttention)                                                   'input_2[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 75, 512)      0           ['multi_head_attention_19[0][0]']\n",
      "                                                                                                  \n",
      " add_normalization_32 (AddNorma  (None, 75, 512)     1024        ['input_2[0][0]',                \n",
      " lization)                                                        'dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_20 (Multi  (None, 75, 512)     131776      ['add_normalization_32[0][0]',   \n",
      " HeadAttention)                                                   'input_2[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 75, 512)      0           ['multi_head_attention_20[0][0]']\n",
      "                                                                                                  \n",
      " add_normalization_33 (AddNorma  (None, 75, 512)     1024        ['add_normalization_32[0][0]',   \n",
      " lization)                                                        'dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " feed_forward_13 (FeedForward)  (None, 75, 512)      2099712     ['add_normalization_33[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 75, 512)      0           ['feed_forward_13[0][0]']        \n",
      "                                                                                                  \n",
      " add_normalization_34 (AddNorma  (None, 75, 512)     1024        ['add_normalization_33[0][0]',   \n",
      " lization)                                                        'dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,366,336\n",
      "Trainable params: 2,366,336\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
    "decoder.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathering and cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaned data available here:\n",
    "#https://github.com/Rishav09/Neural-Machine-Translation-System/blob/master/english-german-both.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German-English sentence pairs for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy.random import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    " return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_clean_sentences('english-german-large.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load, dump, HIGHEST_PROTOCOL\n",
    "from numpy.random import shuffle\n",
    "from numpy import savetxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    " def __init__(self, **kwargs):\n",
    "    super(PrepareDataset, self).__init__(**kwargs)\n",
    "    self.n_sentences = 100000  # Number of sentences to include in the dataset\n",
    "    self.train_split = 0.8  # Ratio of the training data split\n",
    "    self.val_split = 0.1 # Ratio of th evalidation data split\n",
    " \n",
    " # Fit a tokenizer\n",
    " def create_tokenizer(self, dataset):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(dataset)\n",
    " \n",
    "    return tokenizer\n",
    " \n",
    " def find_seq_length(self, dataset):\n",
    "    return max(len(seq.split()) for seq in dataset)\n",
    " \n",
    " def find_vocab_size(self, tokenizer, dataset):\n",
    "    tokenizer.fit_on_texts(dataset)\n",
    " \n",
    "    return len(tokenizer.word_index) + 1\n",
    "\n",
    "   # Encode and pad the input sequences\n",
    " def encode_pad(self, dataset, tokenizer, seq_length):\n",
    "      x = tokenizer.texts_to_sequences(dataset)\n",
    "      x = pad_sequences(x, maxlen=seq_length, padding = 'post')\n",
    "      x = convert_to_tensor(x, dtype=int64)\n",
    "\n",
    "      return x\n",
    " def save_tokenizer(self, tokenizer, name):\n",
    "   with open(name + '_tokenizer.pkl','wb') as handle:\n",
    "      dump(tokenizer, handle, protocol=HIGHEST_PROTOCOL)\n",
    " \n",
    " def __call__(self, filename, **kwargs):\n",
    "    # Load a clean dataset\n",
    "    clean_dataset = load(open(filename, 'rb'))\n",
    "    \n",
    "    # Reduce dataset size\n",
    "    dataset = clean_dataset[:self.n_sentences, :]\n",
    "    \n",
    "    # Include start and end of string tokens\n",
    "    for i in range(dataset[:, 0].size):\n",
    "        dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n",
    "        dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n",
    "    \n",
    "    # Random shuffle the dataset\n",
    "    shuffle(dataset)\n",
    "    \n",
    "    # Split the dataset\n",
    "    train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "    val = dataset[int(self.n_sentences*self.train_split):int(self.n_sentences*(1-self.val_split))]\n",
    "    test = dataset[int(self.n_sentences*(1-self.val_split)):]\n",
    "       \n",
    "    # Prepare tokenizer for the encoder input\n",
    "    enc_tokenizer = self.create_tokenizer(dataset[:, 0])\n",
    "    enc_seq_length = self.find_seq_length(dataset[:, 0])\n",
    "    enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "    # Prepare tokenizer for the decoder input\n",
    "    dec_tokenizer = self.create_tokenizer(dataset[:, 1])\n",
    "    dec_seq_length = self.find_seq_length(dataset[:, 1])\n",
    "    dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "    \n",
    "    # Encode and pad the training input\n",
    "    trainX = self.encode_pad(train[:, 0], enc_tokenizer, enc_seq_length)\n",
    "    trainY = self.encode_pad(train[:, 1], dec_tokenizer, dec_seq_length)\n",
    "\n",
    "   # Encode and pad the validation input\n",
    "    valX = self.encode_pad(val[:, 0], enc_tokenizer, enc_seq_length)\n",
    "    valY = self.encode_pad(val[:, 1], dec_tokenizer, dec_seq_length)\n",
    "\n",
    "   # Save the encoder tokenizer\n",
    "    self.save_tokenizer(enc_tokenizer, 'enc')\n",
    "\n",
    "   # Save the decoder tokenizer\n",
    "    self.save_tokenizer(dec_tokenizer, 'dec')\n",
    "\n",
    "   # Save the testing dataset into a text file\n",
    "    savetxt('test_dataset.txt', test, fmt='%s')\n",
    "\n",
    "    return trainX, trainY, valX, valY, train, val, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from keras.metrics import Mean\n",
    "from tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, TensorSpec, function, int64\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    " \n",
    "# Define the training parameters\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps = 4000, **kwargs):\n",
    "        super(LRScheduler, self).__init__(**kwargs)\n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        # Linearly increasing the learning rate for the first warmup_steps and then decreasing\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps  ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(LRScheduler(d_model),beta_1, beta_2, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 19 10301 19319\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training dataset\n",
    "dataset = PrepareDataset()\n",
    "trainX, trainY, valX, valY, train_orig, val_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-large.pkl')\n",
    " \n",
    "print(enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset batches\n",
    "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    " \n",
    "# Prepare the validation dataset batches\n",
    "val_dataset = data.Dataset.from_tensor_slices((valX, valY))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 5  3 24 ... 22 89  5], shape=(80000,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(trainX[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n , dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function\n",
    "def loss_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the computation of loss\n",
    "    padding_mask = math.logical_not(equal(target, 0))\n",
    "    padding_mask = cast(padding_mask, float32)\n",
    " \n",
    "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
    "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n",
    " \n",
    "    # Compute the mean loss over the unmasked values\n",
    "    return reduce_sum(loss) / reduce_sum(padding_mask)\n",
    " \n",
    " \n",
    "# Defining the accuracy function\n",
    "def accuracy_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the computation of accuracy\n",
    "    padding_mask = math.logical_not(equal(target, 0))\n",
    " \n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2))\n",
    "    accuracy = math.logical_and(padding_mask, accuracy)\n",
    " \n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    padding_mask = cast(padding_mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    " \n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(padding_mask)\n",
    " \n",
    " \n",
    "# Include metrics monitoring\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = Mean(name='train_accuracy')\n",
    "val_loss = Mean(name='val_loss')\n",
    "val_accuracy = Mean(name='val_accuracy')\n",
    " \n",
    "# Create a checkpoint object and manager to manage multiple checkpoints\n",
    "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
    "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=None)\n",
    "\n",
    "#Initialize dictionaries to store the training and validation metrics\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "train_accuracy_dict = {}\n",
    "val_accuracy_dict = {}\n",
    "\n",
    "# Speeding up the training process\n",
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    " \n",
    "        # Run the forward pass of the model to generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    " \n",
    "        # Compute the training loss\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    " \n",
    "        # Compute the training accuracy\n",
    "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
    " \n",
    "    # Retrieve gradients of the trainable variables with respect to the training loss\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    " \n",
    "    # Update the values of the trainable variables by gradient descent\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 21\n",
      "Epoch 21 Step 0 Loss 0.6587 Accuracy 0.8519\n",
      "Epoch 21 Step 50 Loss 0.6977 Accuracy 0.8482\n",
      "Epoch 21 Step 100 Loss 0.6986 Accuracy 0.8478\n",
      "Epoch 21 Step 150 Loss 0.7095 Accuracy 0.8460\n",
      "Epoch 21 Step 200 Loss 0.7128 Accuracy 0.8463\n",
      "Epoch 21 Step 250 Loss 0.7133 Accuracy 0.8459\n",
      "Epoch 21 Step 300 Loss 0.7108 Accuracy 0.8462\n",
      "Epoch 21 Step 350 Loss 0.7088 Accuracy 0.8468\n",
      "Epoch 21 Step 400 Loss 0.7074 Accuracy 0.8471\n",
      "Epoch 21 Step 450 Loss 0.7043 Accuracy 0.8473\n",
      "Epoch 21 Step 500 Loss 0.7030 Accuracy 0.8473\n",
      "Epoch 21 Step 550 Loss 0.7034 Accuracy 0.8472\n",
      "Epoch 21 Step 600 Loss 0.7034 Accuracy 0.8473\n",
      "Epoch 21 Step 650 Loss 0.7031 Accuracy 0.8477\n",
      "Epoch 21 Step 700 Loss 0.7021 Accuracy 0.8482\n",
      "Epoch 21 Step 750 Loss 0.7010 Accuracy 0.8482\n",
      "Epoch 21 Step 800 Loss 0.7000 Accuracy 0.8484\n",
      "Epoch 21 Step 850 Loss 0.7004 Accuracy 0.8482\n",
      "Epoch 21 Step 900 Loss 0.7001 Accuracy 0.8482\n",
      "Epoch 21 Step 950 Loss 0.6986 Accuracy 0.8482\n",
      "Epoch 21 Step 1000 Loss 0.6978 Accuracy 0.8485\n",
      "Epoch 21 Step 1050 Loss 0.6959 Accuracy 0.8488\n",
      "Epoch 21 Step 1100 Loss 0.6957 Accuracy 0.8488\n",
      "Epoch 21 Step 1150 Loss 0.6939 Accuracy 0.8490\n",
      "Epoch 21 Step 1200 Loss 0.6938 Accuracy 0.8490\n",
      "Epoch 21: Training Loss 0.6944, Training Accuracy 0.8490, Validation Loss 2.0247, Validation Accuracy 0.7070\n",
      "\n",
      "Start of epoch 22\n",
      "Epoch 22 Step 0 Loss 0.6281 Accuracy 0.8623\n",
      "Epoch 22 Step 50 Loss 0.6731 Accuracy 0.8536\n",
      "Epoch 22 Step 100 Loss 0.6771 Accuracy 0.8535\n",
      "Epoch 22 Step 150 Loss 0.6834 Accuracy 0.8522\n",
      "Epoch 22 Step 200 Loss 0.6822 Accuracy 0.8524\n",
      "Epoch 22 Step 250 Loss 0.6822 Accuracy 0.8526\n",
      "Epoch 22 Step 300 Loss 0.6779 Accuracy 0.8530\n",
      "Epoch 22 Step 350 Loss 0.6760 Accuracy 0.8531\n",
      "Epoch 22 Step 400 Loss 0.6753 Accuracy 0.8531\n",
      "Epoch 22 Step 450 Loss 0.6744 Accuracy 0.8531\n",
      "Epoch 22 Step 500 Loss 0.6755 Accuracy 0.8531\n",
      "Epoch 22 Step 550 Loss 0.6783 Accuracy 0.8528\n",
      "Epoch 22 Step 600 Loss 0.6786 Accuracy 0.8528\n",
      "Epoch 22 Step 650 Loss 0.6787 Accuracy 0.8530\n",
      "Epoch 22 Step 700 Loss 0.6771 Accuracy 0.8532\n",
      "Epoch 22 Step 750 Loss 0.6752 Accuracy 0.8536\n",
      "Epoch 22 Step 800 Loss 0.6741 Accuracy 0.8536\n",
      "Epoch 22 Step 850 Loss 0.6741 Accuracy 0.8536\n",
      "Epoch 22 Step 900 Loss 0.6732 Accuracy 0.8537\n",
      "Epoch 22 Step 950 Loss 0.6711 Accuracy 0.8539\n",
      "Epoch 22 Step 1000 Loss 0.6705 Accuracy 0.8540\n",
      "Epoch 22 Step 1050 Loss 0.6690 Accuracy 0.8542\n",
      "Epoch 22 Step 1100 Loss 0.6698 Accuracy 0.8541\n",
      "Epoch 22 Step 1150 Loss 0.6685 Accuracy 0.8545\n",
      "Epoch 22 Step 1200 Loss 0.6692 Accuracy 0.8545\n",
      "Epoch 22: Training Loss 0.6698, Training Accuracy 0.8545, Validation Loss 2.0398, Validation Accuracy 0.7090\n",
      "\n",
      "Start of epoch 23\n",
      "Epoch 23 Step 0 Loss 0.6242 Accuracy 0.8779\n",
      "Epoch 23 Step 50 Loss 0.6494 Accuracy 0.8603\n",
      "Epoch 23 Step 100 Loss 0.6590 Accuracy 0.8585\n",
      "Epoch 23 Step 150 Loss 0.6607 Accuracy 0.8573\n",
      "Epoch 23 Step 200 Loss 0.6566 Accuracy 0.8580\n",
      "Epoch 23 Step 250 Loss 0.6553 Accuracy 0.8581\n",
      "Epoch 23 Step 300 Loss 0.6511 Accuracy 0.8589\n",
      "Epoch 23 Step 350 Loss 0.6494 Accuracy 0.8591\n",
      "Epoch 23 Step 400 Loss 0.6501 Accuracy 0.8591\n",
      "Epoch 23 Step 450 Loss 0.6507 Accuracy 0.8591\n",
      "Epoch 23 Step 500 Loss 0.6530 Accuracy 0.8586\n",
      "Epoch 23 Step 550 Loss 0.6553 Accuracy 0.8583\n",
      "Epoch 23 Step 600 Loss 0.6544 Accuracy 0.8583\n",
      "Epoch 23 Step 650 Loss 0.6532 Accuracy 0.8585\n",
      "Epoch 23 Step 700 Loss 0.6517 Accuracy 0.8589\n",
      "Epoch 23 Step 750 Loss 0.6497 Accuracy 0.8593\n",
      "Epoch 23 Step 800 Loss 0.6482 Accuracy 0.8595\n",
      "Epoch 23 Step 850 Loss 0.6479 Accuracy 0.8594\n",
      "Epoch 23 Step 900 Loss 0.6472 Accuracy 0.8594\n",
      "Epoch 23 Step 950 Loss 0.6461 Accuracy 0.8595\n",
      "Epoch 23 Step 1000 Loss 0.6459 Accuracy 0.8596\n",
      "Epoch 23 Step 1050 Loss 0.6451 Accuracy 0.8598\n",
      "Epoch 23 Step 1100 Loss 0.6465 Accuracy 0.8597\n",
      "Epoch 23 Step 1150 Loss 0.6456 Accuracy 0.8599\n",
      "Epoch 23 Step 1200 Loss 0.6461 Accuracy 0.8598\n",
      "Epoch 23: Training Loss 0.6463, Training Accuracy 0.8598, Validation Loss 2.0593, Validation Accuracy 0.7086\n",
      "\n",
      "Start of epoch 24\n",
      "Epoch 24 Step 0 Loss 0.6264 Accuracy 0.8597\n",
      "Epoch 24 Step 50 Loss 0.6212 Accuracy 0.8637\n",
      "Epoch 24 Step 100 Loss 0.6285 Accuracy 0.8637\n",
      "Epoch 24 Step 150 Loss 0.6298 Accuracy 0.8627\n",
      "Epoch 24 Step 200 Loss 0.6307 Accuracy 0.8631\n",
      "Epoch 24 Step 250 Loss 0.6311 Accuracy 0.8629\n",
      "Epoch 24 Step 300 Loss 0.6283 Accuracy 0.8635\n",
      "Epoch 24 Step 350 Loss 0.6276 Accuracy 0.8639\n",
      "Epoch 24 Step 400 Loss 0.6293 Accuracy 0.8634\n",
      "Epoch 24 Step 450 Loss 0.6292 Accuracy 0.8636\n",
      "Epoch 24 Step 500 Loss 0.6301 Accuracy 0.8634\n",
      "Epoch 24 Step 550 Loss 0.6314 Accuracy 0.8631\n",
      "Epoch 24 Step 600 Loss 0.6301 Accuracy 0.8633\n",
      "Epoch 24 Step 650 Loss 0.6297 Accuracy 0.8634\n",
      "Epoch 24 Step 700 Loss 0.6281 Accuracy 0.8638\n",
      "Epoch 24 Step 750 Loss 0.6272 Accuracy 0.8637\n",
      "Epoch 24 Step 800 Loss 0.6257 Accuracy 0.8638\n",
      "Epoch 24 Step 850 Loss 0.6255 Accuracy 0.8637\n",
      "Epoch 24 Step 900 Loss 0.6254 Accuracy 0.8637\n",
      "Epoch 24 Step 950 Loss 0.6244 Accuracy 0.8638\n",
      "Epoch 24 Step 1000 Loss 0.6246 Accuracy 0.8638\n",
      "Epoch 24 Step 1050 Loss 0.6240 Accuracy 0.8640\n",
      "Epoch 24 Step 1100 Loss 0.6254 Accuracy 0.8638\n",
      "Epoch 24 Step 1150 Loss 0.6247 Accuracy 0.8640\n",
      "Epoch 24 Step 1200 Loss 0.6252 Accuracy 0.8639\n",
      "Epoch 24: Training Loss 0.6249, Training Accuracy 0.8640, Validation Loss 2.0721, Validation Accuracy 0.7107\n",
      "\n",
      "Start of epoch 25\n",
      "Epoch 25 Step 0 Loss 0.6013 Accuracy 0.8675\n",
      "Epoch 25 Step 50 Loss 0.6035 Accuracy 0.8666\n",
      "Epoch 25 Step 100 Loss 0.6101 Accuracy 0.8672\n",
      "Epoch 25 Step 150 Loss 0.6142 Accuracy 0.8656\n",
      "Epoch 25 Step 200 Loss 0.6139 Accuracy 0.8662\n",
      "Epoch 25 Step 250 Loss 0.6152 Accuracy 0.8659\n",
      "Epoch 25 Step 300 Loss 0.6134 Accuracy 0.8663\n",
      "Epoch 25 Step 350 Loss 0.6127 Accuracy 0.8668\n",
      "Epoch 25 Step 400 Loss 0.6131 Accuracy 0.8666\n",
      "Epoch 25 Step 450 Loss 0.6124 Accuracy 0.8668\n",
      "Epoch 25 Step 500 Loss 0.6121 Accuracy 0.8667\n",
      "Epoch 25 Step 550 Loss 0.6128 Accuracy 0.8665\n",
      "Epoch 25 Step 600 Loss 0.6116 Accuracy 0.8666\n",
      "Epoch 25 Step 650 Loss 0.6105 Accuracy 0.8669\n",
      "Epoch 25 Step 700 Loss 0.6085 Accuracy 0.8672\n",
      "Epoch 25 Step 750 Loss 0.6065 Accuracy 0.8676\n",
      "Epoch 25 Step 800 Loss 0.6055 Accuracy 0.8676\n",
      "Epoch 25 Step 850 Loss 0.6056 Accuracy 0.8677\n",
      "Epoch 25 Step 900 Loss 0.6063 Accuracy 0.8676\n",
      "Epoch 25 Step 950 Loss 0.6056 Accuracy 0.8678\n",
      "Epoch 25 Step 1000 Loss 0.6059 Accuracy 0.8679\n",
      "Epoch 25 Step 1050 Loss 0.6050 Accuracy 0.8681\n",
      "Epoch 25 Step 1100 Loss 0.6060 Accuracy 0.8680\n",
      "Epoch 25 Step 1150 Loss 0.6046 Accuracy 0.8684\n",
      "Epoch 25 Step 1200 Loss 0.6050 Accuracy 0.8682\n",
      "Epoch 25: Training Loss 0.6050, Training Accuracy 0.8683, Validation Loss 2.0727, Validation Accuracy 0.7115\n",
      "Saved checkpoint at epoch 25\n",
      "\n",
      "Start of epoch 26\n",
      "Epoch 26 Step 0 Loss 0.5772 Accuracy 0.8701\n",
      "Epoch 26 Step 50 Loss 0.5838 Accuracy 0.8728\n",
      "Epoch 26 Step 100 Loss 0.5903 Accuracy 0.8725\n",
      "Epoch 26 Step 150 Loss 0.5961 Accuracy 0.8702\n",
      "Epoch 26 Step 200 Loss 0.5984 Accuracy 0.8699\n",
      "Epoch 26 Step 250 Loss 0.5975 Accuracy 0.8704\n",
      "Epoch 26 Step 300 Loss 0.5952 Accuracy 0.8704\n",
      "Epoch 26 Step 350 Loss 0.5944 Accuracy 0.8707\n",
      "Epoch 26 Step 400 Loss 0.5946 Accuracy 0.8708\n",
      "Epoch 26 Step 450 Loss 0.5934 Accuracy 0.8708\n",
      "Epoch 26 Step 500 Loss 0.5928 Accuracy 0.8707\n",
      "Epoch 26 Step 550 Loss 0.5935 Accuracy 0.8707\n",
      "Epoch 26 Step 600 Loss 0.5910 Accuracy 0.8711\n",
      "Epoch 26 Step 650 Loss 0.5894 Accuracy 0.8714\n",
      "Epoch 26 Step 700 Loss 0.5881 Accuracy 0.8718\n",
      "Epoch 26 Step 750 Loss 0.5867 Accuracy 0.8720\n",
      "Epoch 26 Step 800 Loss 0.5867 Accuracy 0.8721\n",
      "Epoch 26 Step 850 Loss 0.5878 Accuracy 0.8720\n",
      "Epoch 26 Step 900 Loss 0.5888 Accuracy 0.8718\n",
      "Epoch 26 Step 950 Loss 0.5881 Accuracy 0.8718\n",
      "Epoch 26 Step 1000 Loss 0.5879 Accuracy 0.8719\n",
      "Epoch 26 Step 1050 Loss 0.5874 Accuracy 0.8720\n",
      "Epoch 26 Step 1100 Loss 0.5889 Accuracy 0.8717\n",
      "Epoch 26 Step 1150 Loss 0.5880 Accuracy 0.8720\n",
      "Epoch 26 Step 1200 Loss 0.5887 Accuracy 0.8717\n",
      "Epoch 26: Training Loss 0.5890, Training Accuracy 0.8717, Validation Loss 2.0813, Validation Accuracy 0.7105\n",
      "\n",
      "Start of epoch 27\n",
      "Epoch 27 Step 0 Loss 0.5428 Accuracy 0.8857\n",
      "Epoch 27 Step 50 Loss 0.5803 Accuracy 0.8746\n",
      "Epoch 27 Step 100 Loss 0.5793 Accuracy 0.8752\n",
      "Epoch 27 Step 150 Loss 0.5832 Accuracy 0.8738\n",
      "Epoch 27 Step 200 Loss 0.5841 Accuracy 0.8737\n",
      "Epoch 27 Step 250 Loss 0.5817 Accuracy 0.8743\n",
      "Epoch 27 Step 300 Loss 0.5783 Accuracy 0.8747\n",
      "Epoch 27 Step 350 Loss 0.5777 Accuracy 0.8752\n",
      "Epoch 27 Step 400 Loss 0.5788 Accuracy 0.8753\n",
      "Epoch 27 Step 450 Loss 0.5764 Accuracy 0.8755\n",
      "Epoch 27 Step 500 Loss 0.5762 Accuracy 0.8754\n",
      "Epoch 27 Step 550 Loss 0.5759 Accuracy 0.8752\n",
      "Epoch 27 Step 600 Loss 0.5736 Accuracy 0.8754\n",
      "Epoch 27 Step 650 Loss 0.5735 Accuracy 0.8755\n",
      "Epoch 27 Step 700 Loss 0.5731 Accuracy 0.8756\n",
      "Epoch 27 Step 750 Loss 0.5729 Accuracy 0.8756\n",
      "Epoch 27 Step 800 Loss 0.5718 Accuracy 0.8758\n",
      "Epoch 27 Step 850 Loss 0.5723 Accuracy 0.8758\n",
      "Epoch 27 Step 900 Loss 0.5726 Accuracy 0.8758\n",
      "Epoch 27 Step 950 Loss 0.5715 Accuracy 0.8758\n",
      "Epoch 27 Step 1000 Loss 0.5714 Accuracy 0.8758\n",
      "Epoch 27 Step 1050 Loss 0.5708 Accuracy 0.8759\n",
      "Epoch 27 Step 1100 Loss 0.5716 Accuracy 0.8760\n",
      "Epoch 27 Step 1150 Loss 0.5703 Accuracy 0.8762\n",
      "Epoch 27 Step 1200 Loss 0.5710 Accuracy 0.8761\n",
      "Epoch 27: Training Loss 0.5714, Training Accuracy 0.8761, Validation Loss 2.0842, Validation Accuracy 0.7128\n",
      "\n",
      "Start of epoch 28\n",
      "Epoch 28 Step 0 Loss 0.5542 Accuracy 0.8831\n",
      "Epoch 28 Step 50 Loss 0.5625 Accuracy 0.8813\n",
      "Epoch 28 Step 100 Loss 0.5635 Accuracy 0.8796\n",
      "Epoch 28 Step 150 Loss 0.5691 Accuracy 0.8767\n",
      "Epoch 28 Step 200 Loss 0.5689 Accuracy 0.8765\n",
      "Epoch 28 Step 250 Loss 0.5675 Accuracy 0.8766\n",
      "Epoch 28 Step 300 Loss 0.5656 Accuracy 0.8768\n",
      "Epoch 28 Step 350 Loss 0.5652 Accuracy 0.8772\n",
      "Epoch 28 Step 400 Loss 0.5636 Accuracy 0.8774\n",
      "Epoch 28 Step 450 Loss 0.5616 Accuracy 0.8775\n",
      "Epoch 28 Step 500 Loss 0.5605 Accuracy 0.8776\n",
      "Epoch 28 Step 550 Loss 0.5606 Accuracy 0.8774\n",
      "Epoch 28 Step 600 Loss 0.5595 Accuracy 0.8774\n",
      "Epoch 28 Step 650 Loss 0.5602 Accuracy 0.8775\n",
      "Epoch 28 Step 700 Loss 0.5595 Accuracy 0.8780\n",
      "Epoch 28 Step 750 Loss 0.5580 Accuracy 0.8784\n",
      "Epoch 28 Step 800 Loss 0.5575 Accuracy 0.8784\n",
      "Epoch 28 Step 850 Loss 0.5575 Accuracy 0.8785\n",
      "Epoch 28 Step 900 Loss 0.5567 Accuracy 0.8787\n",
      "Epoch 28 Step 950 Loss 0.5563 Accuracy 0.8787\n",
      "Epoch 28 Step 1000 Loss 0.5564 Accuracy 0.8789\n",
      "Epoch 28 Step 1050 Loss 0.5560 Accuracy 0.8790\n",
      "Epoch 28 Step 1100 Loss 0.5571 Accuracy 0.8789\n",
      "Epoch 28 Step 1150 Loss 0.5563 Accuracy 0.8791\n",
      "Epoch 28 Step 1200 Loss 0.5573 Accuracy 0.8788\n",
      "Epoch 28: Training Loss 0.5578, Training Accuracy 0.8787, Validation Loss 2.0850, Validation Accuracy 0.7118\n",
      "\n",
      "Start of epoch 29\n",
      "Epoch 29 Step 0 Loss 0.5265 Accuracy 0.8831\n",
      "Epoch 29 Step 50 Loss 0.5442 Accuracy 0.8821\n",
      "Epoch 29 Step 100 Loss 0.5474 Accuracy 0.8821\n",
      "Epoch 29 Step 150 Loss 0.5554 Accuracy 0.8806\n",
      "Epoch 29 Step 200 Loss 0.5543 Accuracy 0.8817\n",
      "Epoch 29 Step 250 Loss 0.5518 Accuracy 0.8824\n",
      "Epoch 29 Step 300 Loss 0.5484 Accuracy 0.8829\n",
      "Epoch 29 Step 350 Loss 0.5474 Accuracy 0.8826\n",
      "Epoch 29 Step 400 Loss 0.5452 Accuracy 0.8825\n",
      "Epoch 29 Step 450 Loss 0.5435 Accuracy 0.8824\n",
      "Epoch 29 Step 500 Loss 0.5442 Accuracy 0.8822\n",
      "Epoch 29 Step 550 Loss 0.5458 Accuracy 0.8817\n",
      "Epoch 29 Step 600 Loss 0.5453 Accuracy 0.8818\n",
      "Epoch 29 Step 650 Loss 0.5465 Accuracy 0.8819\n",
      "Epoch 29 Step 700 Loss 0.5454 Accuracy 0.8823\n",
      "Epoch 29 Step 750 Loss 0.5442 Accuracy 0.8824\n",
      "Epoch 29 Step 800 Loss 0.5434 Accuracy 0.8825\n",
      "Epoch 29 Step 850 Loss 0.5433 Accuracy 0.8823\n",
      "Epoch 29 Step 900 Loss 0.5436 Accuracy 0.8823\n",
      "Epoch 29 Step 950 Loss 0.5434 Accuracy 0.8824\n",
      "Epoch 29 Step 1000 Loss 0.5441 Accuracy 0.8824\n",
      "Epoch 29 Step 1050 Loss 0.5438 Accuracy 0.8825\n",
      "Epoch 29 Step 1100 Loss 0.5453 Accuracy 0.8823\n",
      "Epoch 29 Step 1150 Loss 0.5444 Accuracy 0.8825\n",
      "Epoch 29 Step 1200 Loss 0.5450 Accuracy 0.8824\n",
      "Epoch 29: Training Loss 0.5453, Training Accuracy 0.8825, Validation Loss 2.0954, Validation Accuracy 0.7134\n",
      "\n",
      "Start of epoch 30\n",
      "Epoch 30 Step 0 Loss 0.5179 Accuracy 0.8909\n",
      "Epoch 30 Step 50 Loss 0.5194 Accuracy 0.8877\n",
      "Epoch 30 Step 100 Loss 0.5270 Accuracy 0.8858\n",
      "Epoch 30 Step 150 Loss 0.5377 Accuracy 0.8841\n",
      "Epoch 30 Step 200 Loss 0.5369 Accuracy 0.8837\n",
      "Epoch 30 Step 250 Loss 0.5361 Accuracy 0.8835\n",
      "Epoch 30 Step 300 Loss 0.5326 Accuracy 0.8845\n",
      "Epoch 30 Step 350 Loss 0.5323 Accuracy 0.8842\n",
      "Epoch 30 Step 400 Loss 0.5311 Accuracy 0.8844\n",
      "Epoch 30 Step 450 Loss 0.5308 Accuracy 0.8847\n",
      "Epoch 30 Step 500 Loss 0.5328 Accuracy 0.8844\n",
      "Epoch 30 Step 550 Loss 0.5351 Accuracy 0.8841\n",
      "Epoch 30 Step 600 Loss 0.5350 Accuracy 0.8841\n",
      "Epoch 30 Step 650 Loss 0.5354 Accuracy 0.8842\n",
      "Epoch 30 Step 700 Loss 0.5333 Accuracy 0.8846\n",
      "Epoch 30 Step 750 Loss 0.5313 Accuracy 0.8849\n",
      "Epoch 30 Step 800 Loss 0.5304 Accuracy 0.8852\n",
      "Epoch 30 Step 850 Loss 0.5303 Accuracy 0.8854\n",
      "Epoch 30 Step 900 Loss 0.5308 Accuracy 0.8855\n",
      "Epoch 30 Step 950 Loss 0.5310 Accuracy 0.8854\n",
      "Epoch 30 Step 1000 Loss 0.5315 Accuracy 0.8854\n",
      "Epoch 30 Step 1050 Loss 0.5314 Accuracy 0.8855\n",
      "Epoch 30 Step 1100 Loss 0.5333 Accuracy 0.8852\n",
      "Epoch 30 Step 1150 Loss 0.5326 Accuracy 0.8853\n",
      "Epoch 30 Step 1200 Loss 0.5333 Accuracy 0.8852\n",
      "Epoch 30: Training Loss 0.5334, Training Accuracy 0.8852, Validation Loss 2.1085, Validation Accuracy 0.7124\n",
      "Saved checkpoint at epoch 30\n",
      "Total time taken: 325.63s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    " \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    " \n",
    "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
    " \n",
    "    start_time = time()\n",
    " \n",
    "    # Iterate over the dataset batches\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    " \n",
    "        # Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = train_batchX[:, 1:]\n",
    "        decoder_input = train_batchY[:, :-1]\n",
    "        decoder_output = train_batchY[:, 1:]\n",
    " \n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    " \n",
    "        if step % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "            # print(\"Samples so far: %s\" % ((step + 1) * batch_size))\n",
    "    \n",
    "    #Run a validation step after every epoch of training\n",
    "    for val_batchX, val_batchY in val_dataset:\n",
    "        \n",
    "        #Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = val_batchX[:, 1:]\n",
    "        decoder_input = val_batchY[:, :-1]\n",
    "        decoder_output = val_batchY[:, 1:]\n",
    "\n",
    "        #Generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=False)\n",
    "\n",
    "        #Compute the validation loss\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "        val_loss(loss)\n",
    "        accuracy = accuracy_fcn(decoder_output,prediction)\n",
    "        val_accuracy(accuracy)\n",
    "\n",
    "    # Print epoch number and loss value at the end of every epoch\n",
    "    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f, Validation Loss %.4f, Validation Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result(), val_loss.result(), val_accuracy.result()))\n",
    " \n",
    "    # Save a checkpoint after every five epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n",
    "\n",
    "        # Save the trained model weights\n",
    "        training_model.save_weights('weights/wghts' + str(epoch+1) + '.ckpt')\n",
    "\n",
    "        train_loss_dict[epoch] = train_loss.result()\n",
    "        train_accuracy_dict[epoch] = train_accuracy.result()\n",
    "        val_loss_dict[epoch] = val_loss.result()\n",
    "        val_accuracy_dict[epoch] = val_accuracy.result()\n",
    "# Save the training loss values\n",
    "with open('./train_loss.pkl', 'wb') as file:\n",
    "    dump(train_loss_dict, file)\n",
    "\n",
    "with open('./train_accuracy.pkl', 'wb') as file:\n",
    "    dump(train_accuracy_dict, file)\n",
    " \n",
    "# Save the validation loss values\n",
    "with open('./val_loss.pkl', 'wb') as file:\n",
    "    dump(val_loss_dict, file)\n",
    "\n",
    "with open('./val_accuracy.pkl', 'wb') as file:\n",
    "    dump(val_accuracy_dict, file)\n",
    " \n",
    "print(\"Total time taken: %.2fs\" % (time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    " \n",
    "# Define the dataset parameters\n",
    "enc_seq_length = 12  # Encoder sequence length\n",
    "dec_seq_length = 19  # Decoder sequence length\n",
    "enc_vocab_size = 10301  # Encoder vocabulary size\n",
    "dec_vocab_size = 19319  # Decoder vocabulary size\n",
    " \n",
    "# Create model\n",
    "inferencing_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translate(Module):\n",
    "    def __init__(self, inferencing_model, **kwargs):\n",
    "        super(Translate, self).__init__(**kwargs)\n",
    "        self.transformer = inferencing_model\n",
    "\n",
    "    def load_tokenizer(self, name):\n",
    "        with open(name, 'rb') as handle:\n",
    "            return load(handle)\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        #modify the sentence to have the start and end tokens\n",
    "        sentence[0] = \"<START> \" + sentence[0] + \" <EOS>\"\n",
    "        #print(sentence)\n",
    "\n",
    "        # Load the tokenizers from the training model\n",
    "        enc_tokenizer = self.load_tokenizer('enc_tokenizer.pkl')\n",
    "        dec_tokenizer = self.load_tokenizer('dec_tokenizer.pkl')\n",
    "\n",
    "        #Tokenize the sentence, pad it, and convert to a tensor\n",
    "        encoder_input = enc_tokenizer.texts_to_sequences(sentence)\n",
    "        encoder_input = pad_sequences(encoder_input, maxlen=enc_seq_length, padding = 'post')\n",
    "        encoder_input = convert_to_tensor(encoder_input, dtype=int64)\n",
    "        print(encoder_input)\n",
    "\n",
    "        output_start = dec_tokenizer.texts_to_sequences(['<START>'])\n",
    "        output_start = convert_to_tensor(output_start[0], dtype=int64)\n",
    "\n",
    "        output_end = dec_tokenizer.texts_to_sequences(['<EOS>'])\n",
    "        output_end = convert_to_tensor(output_end[0], dtype = int64)\n",
    "\n",
    "        #Prepare the output array that will contain the translated text. \n",
    "        # Since you do not know the length of the translated sentence in advance, you will initialize the \n",
    "        # size of the output array to 0, but set its dynamic_size parameter to True so that it may grow past its initial size. \n",
    "        # You will then set the first value in this output array to the <START> token:\n",
    "\n",
    "        decoder_output = tf.TensorArray(dtype=int64, size = 0, dynamic_size=True)\n",
    "        decoder_output = decoder_output.write(0,output_start)\n",
    "\n",
    "        #Iterate up to the sequence max length each time calling the transformer to predict a token\n",
    "        #training is set to false so no dropouts occur\n",
    "        #prediction with the highest score is then selected and written at the next index of hte output array\n",
    "        #for loop breaks when <EOS> is predicted\n",
    "\n",
    "        for i in range(dec_seq_length):\n",
    "\n",
    "            prediction = self.transformer(encoder_input, transpose(decoder_output.stack()), training=False)\n",
    "\n",
    "            prediction = prediction[:, -1,:]\n",
    "\n",
    "            predicted_id = argmax(prediction, axis=-1)\n",
    "            predicted_id = predicted_id[0][tf.newaxis]\n",
    " \n",
    "            decoder_output = decoder_output.write(i + 1, predicted_id)\n",
    "        \n",
    "            if predicted_id == output_end:\n",
    "                break\n",
    "\n",
    "            #decode the predicted tokens into an output list and return it\n",
    "\n",
    "        output = transpose(decoder_output.stack())[0]\n",
    "        output = output.numpy()\n",
    "\n",
    "        output_str = []\n",
    "\n",
    "        # Decode the predicted tokens into an output list\n",
    "        for i in range(output.shape[0]):\n",
    "\n",
    "            key = output[i]\n",
    "            translation = dec_tokenizer.index_word[key]\n",
    "            output_str.append(translation)\n",
    "\n",
    "        return output_str\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1eb61745e20>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferencing_model.load_weights(\"weights\\wghts30.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translate(inferencing_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['i often listen to sad songs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[   1    1    1    4  234  485    6  738 1291    2    2    2]], shape=(1, 12), dtype=int64)\n",
      "['fang', 'an', 'ich', 'es', 'ist', 'oft', 'nun']\n"
     ]
    }
   ],
   "source": [
    "print(translator(sentence)[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2af03d7e22c24bf2e6d190a35b261c407267ffc2225e910495761a73f1323185"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
